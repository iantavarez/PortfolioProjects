import requests
from bs4 import BeautifulSoup
import time

# NBA MVP Predictor - Dataquest
# Add advanced stats to the model to improve the predictions

# we are gonna start in 1980 because that is when the 3 point line was introduced, 
# and steals and blocks were also recorded (in 1974)
years = list(range(1980, 2025))

base_url = "https://www.basketball-reference.com/awards/awards_{}.html"

for year in years:
    url = base_url.format(year)
    data = requests.get(url)

    with open ("{}.html".format(year), "w+") as f:
        f.write(data.text)

    time.sleep(5)

with open("1991.html") as f:
    page = f.read()

soup = BeautifulSoup(page, "html.parser")

soup.find('tr', class_="over_header").decompose()

mvp_table = soup.find(id="mvp")

import pandas as pd

mvp_1991 = pd.read_html(str(mvp_table))[0]

mvp_1991

dfs = []
for year in years:
    with open("{}.html".format(year)) as f:
        page = f.read()
    soup = BeautifulSoup(page, "html.parser")
    soup.find('tr', class_="over_header").decompose()
    mvp_table = soup.find(id="mvp")
    mvp = pd.read_html(str(mvp_table))[0]
    mvp["Year"] = year
    dfs.append(mvp)

dfs

mvps = pd.concat(dfs)
mvps.to_csv("mvps.csv")

from selenium import webdriver

path = "C:/Users/Tavar/Downloads/chromedriver"
driver = webdriver.Chrome()

player_stats_url = "https://www.basketball-reference.com/leagues/NBA_{}_per_game.html"

year = 1980
url_1 = player_stats_url.format(year)
driver.get(url_1)
driver.execute_script("window.scrollTo(1,10000)")
time.sleep(2)
html = driver.page_source

html

with open ("player{}.html".format(year), "w+") as f:
    f.write(html)

for year in years:
    url_1 = player_stats_url.format(year)
    driver.get(url_1)
    driver.execute_script("window.scrollTo(1,10000)")
    time.sleep(5)
    html = driver.page_source

    with open ("player{}.html".format(year), "w+") as f:
        f.write(html)

dfs = []
for year in years:
    with open("player{}.html".format(year)) as f:
        page = f.read()
    soup = BeautifulSoup(page, "html.parser")
    soup.find('tr', class_="thead").decompose()
    player_table = soup.find(id="per_game_stats")
    player = pd.read_html(str(player_table))[0]
    player["Year"] = year
    dfs.append(player)

players = pd.concat(dfs)

players

players.to_csv("players.csv")

from selenium import webdriver

path = "C:/Users/Tavar/Downloads/chromedriver"
driver = webdriver.Chrome()

advanced_stats_url = "https://www.basketball-reference.com/leagues/NBA_{}_advanced.html"

year = 1980
url_1 = advanced_stats_url.format(year)
driver.get(url_1)
driver.execute_script("window.scrollTo(1,10000)")
time.sleep(2)
html = driver.page_source

html

with open ("advanced{}.html".format(year), "w+") as f:
    f.write(html)

for year in years:
    url_1 = advanced_stats_url.format(year)
    driver.get(url_1)
    driver.execute_script("window.scrollTo(1,10000)")
    time.sleep(5)
    html = driver.page_source

    with open ("advanced{}.html".format(year), "w+") as f:
        f.write(html)

dfs = []
for year in years:
    with open("advanced{}.html".format(year)) as f:
        page = f.read()
    soup = BeautifulSoup(page, "html.parser")
    soup.find('tr', class_="thead").decompose()
    advanced_table = soup.find(id="advanced_stats")
    advanced = pd.read_html(str(advanced_table))[0]
    advanced["Year"] = year
    dfs.append(advanced)

advanced_stats = pd.concat(dfs)

advanced_stats

advanced_stats.to_csv("advanced.csv")

team_stats_url = "https://www.basketball-reference.com/leagues/NBA_{}_standings.html"

for year in years:
    url_2 = team_stats_url.format(year)
    data = requests.get(url_2)

    with open ("team{}.html".format(year), "w+") as f:
        f.write(data.text)

    time.sleep(5)

dfs = []
for year in years:
    with open("team{}.html".format(year)) as f:
        page = f.read()
    soup = BeautifulSoup(page, "html.parser")
    soup.find('tr', class_="thead").decompose()
    team_table = soup.find(id="divs_standings_E")
    team = pd.read_html(str(team_table))[0]
    team["Year"] = year
    team["Team"] = team["Eastern Conference"]
    del team["Eastern Conference"]
    dfs.append(team)

    soup = BeautifulSoup(page, "html.parser")
    soup.find('tr', class_="thead").decompose()
    team_table = soup.find(id="divs_standings_W")
    team = pd.read_html(str(team_table))[0]
    team["Year"] = year
    team["Team"] = team["Western Conference"]
    del team["Western Conference"]
    dfs.append(team)


teams = pd.concat(dfs)

teams

teams.to_csv("teams.csv")

mvps = pd.read_csv("mvps.csv")

mvps

mvps = mvps[['Player', 'Year', 'Pts Won', 'Pts Max', 'Share',]]

mvps.head()

players = pd.read_csv("players.csv")

players

del players["Rk"]
del players["Unnamed: 0"]

players

players["Player"].head(50)

players["Player"] = players["Player"].str.replace("*", "", regex=False)

players["Player"].head(50)

advanced_stats = pd.read_csv("advanced.csv")
del advanced_stats["Unnamed: 0"]
advanced_stats["Player"] = advanced_stats["Player"].str.replace("*", "", regex=False)
advanced_stats = advanced_stats.groupby(["Rk", "Year"]).apply(single_row) # type: ignore
advanced_stats.index = advanced_stats.index.droplevel()
advanced_stats.index = advanced_stats.index.droplevel()
advanced_stats
advanced_stats.to_csv("advanced.csv")

players.groupby(["Player", "Year"]).get_group(("Greg Anderson", 1991))
players.groupby(["Player", "Year"]).get_group(("George Johnson", 1980))
players = players.groupby(["Rk", "Year"]).apply(single_row) # type: ignore
players

def single_row(df):
    if df.shape[0] == 1:
        return df
    else:
        row = df[df["Tm"] == "TOT"]
        row["Tm"] = df.iloc[-1,:]["Tm"]
        return row

# there's a bug, there are two george johnsons in the data that played
# at the same time
# make an ID for each player?
players = players.groupby(["Player", "Year"]).apply(single_row)

players.head(20)

players.index = players.index.droplevel()
players.index = players.index.droplevel()

players.head(20)

players[players["Player"] == "Charles Barkley"]

players[players["Player"] == "George Johnson"]

combined = players.merge(mvps, how="outer", on=["Player", "Year"])

combined.head(5)

combined[combined["Pts Won"] > 0]

combined[["Pts Won", "Pts Max", "Share"]] = combined[["Pts Won", "Pts Max", "Share"]].fillna(0)

combined

# add this later
del combined["WS"]
del combined["WS/48"]

combined

teams = pd.read_csv("teams.csv")

teams

teams.head(30)

teams = teams[~teams["W"].str.contains("Division")]

teams["Team"] = teams["Team"].str.replace("*", "", regex=False)

teams.head(5)

teams["Team"].unique()

nicknames = {}

with open("nicknames.csv") as f:
    lines = f.readlines()
    for line in lines[1:]:
        abbrev,name = line.replace("\n", "").split(",")
        nicknames[abbrev] = name

nicknames

combined["Team"] = combined["Tm"].map(nicknames)

combined.head(5)

combined[combined["Player"] == "George Johnson"]

stats = combined.merge(teams, how="outer", on=["Team", "Year"])

stats

del stats["Unnamed: 0"]

stats = stats.apply(pd.to_numeric, errors="ignore")

stats.dtypes
advanced_stats.dtypes
advanced_stats = advanced_stats.apply(pd.to_numeric, errors="ignore")
del advanced_stats["Player"]
del advanced_stats["Pos"]
del advanced_stats["Age"]   
del advanced_stats["Tm"]
del advanced_stats["G"]
del advanced_stats["MP"]
advanced_stats.columns
advanced_stats.to_csv("advanced.csv")

stats["GB"].unique()

stats["GB"] = stats["GB"].str.replace("—", "0")

stats["GB"].unique()

stats["GB"] = pd.to_numeric(stats["GB"])

stats.to_csv("stats.csv")

stats = pd.read_csv("stats.csv")

del stats["Unnamed: 0"]

pd.isnull(stats)
pd.isnull(stats).sum()

stats[pd.isnull(stats["3P%"])][["Player", "3PA"]]
stats[pd.isnull(stats["FT%"])][["Player", "FTA"]]

# either fill games started with 0 or remove from model
stats = stats.fillna(0)

stats.columns

stats.to_csv("stats.csv")
stats = pd.read_csv("stats.csv")

stats = stats.merge(advanced_stats, how="outer", on=["Rk", "Year"])
stats.to_csv("stats.csv")

import pandas as pd
stats = pd.read_csv("stats.csv")
stats

del stats["Unnamed: 0.1"]
del stats["Unnamed: 0"]

predictors = ['Age', 'G', 'GS', 'MP', 'FG', 'FGA', 'FG%', '3P', '3PA', '3P%', '2P', '2PA', '2P%', 'eFG%', 'FT', 'FTA', 'FT%', 'ORB', 
'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF', 'PTS', 'Year','W', 'L', 'W/L%', 'GB', 'PS/G', 
'PA/G', 'SRS', 'PER', 'TS%', '3PAr', 'FTr', 'ORB%', 'DRB%', 'TRB%', 
'AST%', 'STL%', 'BLK%', 'TOV%', 'USG%', 'OWS', 'DWS', 'WS', 'WS/48', 
'OBPM', 'DBPM', 'BPM', 'VORP', 'TS_R', 'FG_75', 'FGA_75', 'FG%_75', '3P_75', '3PA_75', '3P%_75', '2P_75', '2PA_75',
'2P%_75', 'FT_75', 'FTA_75', 'FT%_75', 'ORB_75', 'DRB_75', 'TRB_75',
'AST_75', 'STL_75', 'BLK_75', 'TOV_75', 'PF_75', 'PTS_75', 'ORtg_75',
'DRtg_75']

train = stats[stats["Year"] < 2023]
test = stats[stats["Year"] == 2023]

from sklearn.linear_model import Ridge
reg = Ridge(alpha=176)

reg.fit(train[predictors], train["Share"])

predictions = reg.predict(test[predictors])
predictions = pd.DataFrame(predictions, columns=["predictions"], index = test.index)
predictions

combination = pd.concat([test[["Player", "Share"]], predictions], axis = 1)
combination.sort_values("Share", ascending = False).head(10)

from sklearn.metrics import mean_squared_error
mean_squared_error(combination["Share"], combination["predictions"])

combination["Share"].value_counts()

combination = combination.sort_values("Share", ascending = False)
combination["Rk"] = list(range(1, combination.shape[0] + 1))

combination.head(10)

combination = combination.sort_values("predictions", ascending = False)
combination["Predicted_Rk"] = list(range(1, combination.shape[0] + 1))

combination.head(10)

def find_ap(combination):
    actual = combination.sort_values("Share", ascending = False).head(5)
    predicted = combination.sort_values("predictions", ascending = False)
    ps = []
    found = 0
    seen = 1
    for index, row in predicted.iterrows():
        if row["Player"] in actual["Player"].values:
            found += 1
            ps.append(found / seen)
        seen += 1
    return sum(ps) / len(ps)

find_ap(combination)

years = list(range(1980, 2024))

aps = []
all_predictions = []
for year in years[5:]:
    train = stats[stats["Year"] < year]
    test = stats[stats["Year"] == year]
    reg.fit(train[predictors], train["Share"])
    predictions = reg.predict(test[predictors])
    predictions = pd.DataFrame(predictions, columns=["predictions"], index = test.index)
    combination = pd.concat([test[["Player", "Share"]], predictions], axis = 1)
    all_predictions.append(combination)
    aps.append(find_ap(combination))
sum(aps) / len(aps)

def add_ranks(combination):
    combination = combination.sort_values("Share", ascending = False)
    combination["Rk"] = list(range(1, combination.shape[0] + 1))
    combination = combination.sort_values("predictions", ascending = False)
    combination["Predicted_Rk"] = list(range(1, combination.shape[0] + 1))
    combination["Diff"] = combination["Rk"] - combination["Predicted_Rk"]
    return combination

add_ranks(all_predictions[1]).sort_values("Diff", ascending = False)
# this is 1986 season i believe
ranking = add_ranks(all_predictions[1])
ranking[ranking["Rk"] < 6].sort_values("Diff", ascending = False)
# ranking.sort_values("Predicted_Rk").head(10)

def backtest(stats, model, year, predictors):
    aps = []
    all_predictions = []
    for year in years[5:]:
        train = stats[stats["Year"] < year]
        test = stats[stats["Year"] == year]
        model.fit(train[predictors], train["Share"])
        predictions = reg.predict(test[predictors])
        predictions = pd.DataFrame(predictions, columns=["predictions"], index = test.index)
        combination = pd.concat([test[["Player", "Share"]], predictions], axis = 1)
        combination = add_ranks(combination)
        all_predictions.append(combination)
        aps.append(find_ap(combination))
    return sum(aps) / len(aps), aps, pd.concat(all_predictions)

mean_ap, aps, all_predictions = backtest(stats, reg, years[5:], predictors)

mean_ap

all_predictions[all_predictions["Rk"] <= 5].sort_values("Diff").head(10)
# could dig in deeper and look at stats of jason kidd etc. to see why the model is not predicting them as well

pd.concat([pd.Series(reg.coef_), pd.Series(predictors)], axis = 1).sort_values(0, ascending = False)
# what variables are most important to the model

true_shooting_ratio = stats[["TS%", "Year"]].groupby("Year").apply(lambda x: x/x.mean())
true_shooting_ratio
true_shooting_ratio.index = true_shooting_ratio.index.droplevel()
stats[["TS_R"]] = true_shooting_ratio[["TS%"]]
predictors += ["TS_R"]

# i dont really care about the rest of this stuff
stat_ratios = stats[["PTS", "AST", "STL", "BLK", "3P", "Year"]].groupby("Year").apply(lambda x: x/x.mean())

stats
stat_ratios
stat_ratios.index = stat_ratios.index.droplevel()

stats[["PTS_R", "AST_R", "STL_R", "BLK_R", "3P_R"]] = stat_ratios[["PTS", "AST", "STL", "BLK", "3P"]]

stats.head()

predictors += ["PTS_R", "AST_R", "STL_R", "BLK_R", "3P_R"]

mean_ap, aps, all_predictions = backtest(stats, reg, years[5:], predictors)

mean_ap

stats["NPos"] = stats["Pos"].astype("category").cat.codes
stats.head()

stats["NTm"] = stats["Tm"].astype("category").cat.codes
stats["NTm"].value_counts()

from sklearn.ensemble import RandomForestRegressor

rf = RandomForestRegressor(n_estimators=100, random_state=1, min_samples_split=5)

mean_ap, aps, all_predictions = backtest(stats, rf, years[28:], predictors)

mean_ap

mean_ap, aps, all_predictions = backtest(stats, reg, years[28:], predictors)

mean_ap

# Steps 
# do more investigation on the data on what i could add/take away
# generate more predictors within the set
# play around with different machine learning models
# try to get more data from basketball-reference

# get per 100 possession stats and multiply by .75 to get per 75 possession stats
# and adjust for era
# get team ratings and take average of offensive rating 
# and adjust to average offensive rating of the league today

# somehow include voter fatigue in the model?

# testing on 2024 season

from selenium import webdriver

path = "C:/Users/Tavar/Downloads/chromedriver"
driver = webdriver.Chrome()

player_stats_url = "https://www.basketball-reference.com/leagues/NBA_{}_per_game.html"

year = 2024
url_1 = player_stats_url.format(year)
driver.get(url_1)
driver.execute_script("window.scrollTo(1,10000)")
time.sleep(2)
html = driver.page_source

html

with open ("player{}.html".format(year), "w+") as f:
    f.write(html)

dfs = []
with open("player{}.html".format(year)) as f:
    page = f.read()
soup = BeautifulSoup(page, "html.parser")
soup.find('tr', class_="thead").decompose()
player_table = soup.find(id="per_game_stats")
player = pd.read_html(str(player_table))[0]
player["Year"] = year
dfs.append(player)

players_2024 = pd.concat(dfs)

players_2024

advanced_stats_url = "https://www.basketball-reference.com/leagues/NBA_{}_advanced.html"

year = 2024
url_1 = advanced_stats_url.format(year)
driver.get(url_1)
driver.execute_script("window.scrollTo(1,10000)")
time.sleep(2)
html = driver.page_source

html

with open ("advanced{}.html".format(year), "w+") as f:
    f.write(html)

dfs = []
with open("advanced{}.html".format(year)) as f:
    page = f.read()
soup = BeautifulSoup(page, "html.parser")
soup.find('tr', class_="thead").decompose()
advanced_table = soup.find(id="advanced_stats")
advanced = pd.read_html(str(advanced_table))[0]
advanced["Year"] = year
dfs.append(advanced)

advanced_2024 = pd.concat(dfs)

team_stats_url = "https://www.basketball-reference.com/leagues/NBA_{}_standings.html"

url_2 = team_stats_url.format(year)
data = requests.get(url_2)

with open ("team{}.html".format(year), "w+") as f:
    f.write(data.text)


dfs = []
with open("team{}.html".format(year)) as f:
    page = f.read()
soup = BeautifulSoup(page, "html.parser")
soup.find('tr', class_="thead").decompose()
team_table = soup.find(id="divs_standings_E")
team = pd.read_html(str(team_table))[0]
team["Year"] = year
team["Team"] = team["Eastern Conference"]
del team["Eastern Conference"]
dfs.append(team)

soup = BeautifulSoup(page, "html.parser")
soup.find('tr', class_="thead").decompose()
team_table = soup.find(id="divs_standings_W")
team = pd.read_html(str(team_table))[0]
team["Year"] = year
team["Team"] = team["Western Conference"]
del team["Western Conference"]
dfs.append(team)

teams_2024 = pd.concat(dfs)

players_2024["Player"] = players_2024["Player"].str.replace("*", "", regex=False)
advanced_2024["Player"] = advanced_2024["Player"].str.replace("*", "", regex=False)

players_2024 = players_2024.groupby(["Rk", "Year"]).apply(single_row)
advanced_2024 = advanced_2024.groupby(["Rk", "Year"]).apply(single_row)

players_2024.index = players_2024.index.droplevel()
players_2024.index = players_2024.index.droplevel()

advanced_2024.index = advanced_2024.index.droplevel()
advanced_2024.index = advanced_2024.index.droplevel()

teams_2024 = teams_2024[~teams_2024["W"].str.contains("Division")]

teams_2024["Team"] = teams_2024["Team"].str.replace("*", "", regex=False)
for i in range(1, 16):
    teams_2024["Team"] = teams_2024["Team"].str.replace("({})".format(i), "", regex=False)
teams_2024
players_2024

nicknames = {}

with open("nicknames.csv") as f:
    lines = f.readlines()
    for line in lines[1:]:
        abbrev,name = line.replace("\n", "").split(",")
        nicknames[abbrev] = name

players_2024["Team"] = players_2024["Tm"].map(nicknames)

players_2024.columns
teams_2024.columns

players_2024['Team'] = players_2024['Team'].str.strip()
teams_2024['Team'] = teams_2024['Team'].str.strip()

stats_2024 = players_2024.merge(teams_2024, how="outer", on=["Team"])
stats_2024 = stats_2024.apply(pd.to_numeric, errors="ignore")
advanced_2024 = advanced_2024.apply(pd.to_numeric, errors="ignore")
del advanced_2024["Player"]
del advanced_2024["Pos"]
del advanced_2024["Age"]   
del advanced_2024["Tm"]
del advanced_2024["G"]
del advanced_2024["MP"]
stats_2024 = stats_2024.merge(advanced_2024, how="outer", on=["Rk"])

stats_2024["GB"] = stats_2024["GB"].str.replace("—", "0")

stats_2024["GB"] = pd.to_numeric(stats_2024["GB"])

stats_2024 = stats_2024.fillna(0)

del stats_2024["Year_x"]
del stats_2024["Year_y"]
del stats_2024["Unnamed: 19"]
del stats_2024["Unnamed: 24"]
stats_2024.to_csv("stats_2024.csv")

stats = pd.concat([stats, stats_2024])
del stats["Tm"]
stats.to_csv("stats.csv")

stats = pd.read_csv("stats.csv")
stats.columns

train = stats[stats["Year"] < 2024]
test = stats[(stats["Year"] == 2024) & (stats["G"] > 65)]

from sklearn.linear_model import Ridge
reg = Ridge(alpha=176)

reg.fit(train[predictors], train["Share"])

predictions = reg.predict(test[predictors])
predictions = pd.DataFrame(predictions, columns=["predictions"], index = test.index)
predictions

combination = pd.concat([test[["Player", "Share"]], predictions], axis = 1)
combination.sort_values("Share", ascending = False).head(10)

combination = combination.sort_values("Share", ascending = False)
combination["Rk"] = list(range(1, combination.shape[0] + 1))

combination.head(10)

combination = combination.sort_values("predictions", ascending = False)
combination["Predicted_Rk"] = list(range(1, combination.shape[0] + 1))

combination.head(15)

del combination["Share"]
del combination["Rk"]

# make bar chart of top 5/10 players and find a way to convert
# into probabilties

from sklearn.linear_model import RidgeCV

# Define the alphas you want to test
alphas = [174.2, 174.4, 174.6, 174.8, 175, 175.2, 175.4, 175.6, 175.8, 176]

# Create a RidgeCV model
reg = RidgeCV(alphas=alphas, cv=10)

# Fit the model
reg.fit(train[predictors], train["Share"])

# The alpha parameter chosen by cross-validation
print("Best alpha:", reg.alpha_)

# possession data

from selenium import webdriver

path = "C:/Users/Tavar/Downloads/chromedriver"
driver = webdriver.Chrome()

poss_stats_url = "https://www.basketball-reference.com/leagues/NBA_{}_per_poss.html"

for year in years:
    url_1 = poss_stats_url.format(year)
    driver.get(url_1)
    driver.execute_script("window.scrollTo(1,10000)")
    time.sleep(5)
    html = driver.page_source

    with open ("poss{}.html".format(year), "w+") as f:
        f.write(html)

dfs = []
for year in years:
    with open("poss{}.html".format(year)) as f:
        page = f.read()
    soup = BeautifulSoup(page, "html.parser")
    soup.find('tr', class_="thead").decompose()
    poss_table = soup.find(id="per_poss_stats")
    poss = pd.read_html(str(poss_table))[0]
    poss["Year"] = year
    dfs.append(poss)

poss = pd.concat(dfs)

poss["Player"] = poss["Player"].str.replace("*", "", regex=False)
poss = poss.groupby(["Rk", "Year"]).apply(single_row)
poss.index = poss.index.droplevel()
poss.index = poss.index.droplevel()

# REMOVE PERCENTAGES
poss_75 = ['FG', 'FGA', 'FG%', '3P', '3PA', 
           '3P%', '2P', '2PA', '2P%', 'FT', 'FTA', 'FT%', 
           'ORB', 'DRB', 'TRB', 'AST', 'STL', 'BLK', 
           'TOV', 'PF', 'PTS', 'ORtg', 'DRtg']

poss = poss.apply(pd.to_numeric, errors="ignore")
poss.columns
poss[poss_75] = poss[poss_75] * 0.75

del poss["Player"]
del poss["Pos"]
del poss["Age"]   
del poss["Tm"]
del poss["G"]
del poss["GS"]
del poss["MP"]
del poss["Unnamed: 29"]

poss.isna().sum()
poss.fillna(0)

# Assuming cols_to_rename is a list of column names you want to rename
cols_to_rename = ['FG', 'FGA', 'FG%', '3P', '3PA', '3P%', '2P', '2PA', '2P%', 'FT',
       'FTA', 'FT%', 'ORB', 'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF',
       'PTS', 'ORtg', 'DRtg']

poss.rename(columns={col: col + '_75' for col in cols_to_rename}, inplace=True)

stats = stats.merge(poss, how="outer", on=["Rk", "Year"])

poss.to_csv("poss.csv")

stats.to_csv("stats.csv")
